# ML - Transformer
|Paper|Conference|Remarks
|--|--|--|
|[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)|NIPS 2017|1. Propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 2. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Lipschitz Constrained Parameter Initialization for Deep Transformers](https://arxiv.org/abs/1911.03179)|ACL 2020|1. Present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence.|
|[# KERMIT - Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations](https://www.aclweb.org/anthology/2020.emnlp-main.18/)|EMNLP 2020|1. Propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. 2. Showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks.|
|[# Highway Transformer - Self-Gating Enhanced Self-Attentive Networks](https://arxiv.org/abs/2004.08178)|ACL 2020|1. Introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. 2. Showed that SDU leads to a clear margin of convergence speed with gradient descent algorithms.|
|[# HAT - Hardware-Aware Transformers for Efficient Natural Language Processing](https://arxiv.org/abs/2005.14187)|ACL 2020|1. Propose to design Hardware-Aware Transformers (HAT) with neural architecture search. 2. Show that HAT can discover efficient models for different hardware (CPU, GPU, IoT device).|
|[# Funnel-Transformer - Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)|NeurIPS 2020|1. Propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. 2. Shows that, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension.|
|[# FastFormers - Highly Efficient Transformer Models for Natural Language Understanding](https://www.aclweb.org/anthology/2020.sustainlp-1.20/)|SustaiNLP Workshop 2020|1. Present FastFormers, a set of recipes, e.g., knowledge distillation, structured pruning and numerical optimization, to achieve efficient inference-time performance for Transformer-based models on various NLU tasks.|
|[# Fast Transformers with Clustered Attention](https://arxiv.org/abs/2007.04825)|NeurIPS 2020|1. Propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. 2. Propose to use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. 3. our model consistently outperforms vanilla transformers for a given computational budget.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTc1NzY0ODA1MiwtOTAzMTUxMjkyXX0=
-->