# ML - Transformer
|Paper|Conference|Remarks
|--|--|--|
|[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)|NIPS 2017|1. Propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 2. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Lipschitz Constrained Parameter Initialization for Deep Transformers](https://arxiv.org/abs/1911.03179)|ACL 2020|1. Present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence.|
|[# KERMIT - Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations](https://www.aclweb.org/anthology/2020.emnlp-main.18/)|EMNLP 2020|1. Propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. 2. Showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0MjI5MDE1NDIsLTkwMzE1MTI5Ml19
-->