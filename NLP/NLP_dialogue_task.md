# NLP - Dialogue System (Task-oriented)
|Paper|Conference|Remarks
|--|--|--|
|[POMDP-based Statistical Spoken Dialogue Systems: a Review](http://mi.eng.cam.ac.uk/~sjy/papers/ygtw13.pdf)|Proceedings of IEEE 2013|1. By including an explicit Bayesian model of uncertainty and by optimizing the policy via a reward-driven process, partially observable Markov decision processes (POMDPs) provide a data-driven framework for dialog modelling. 2. Provides an overview of the current state of the art in the development of POMDP-based spoken dialog systems.|
|[Learning End-to-End Goal-Oriented Dialog](https://arxiv.org/pdf/1605.07683)|ICLR 2017| 1. Proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. 2. Show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations.|
|[A Network-based End-to-End Trainable Task-oriented Dialogue System](https://arxiv.org/pdf/1604.04562)|EACL 2017| 1. Introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. 2. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand.|
|[Building Task-Oriented Dialogue Systems for Online Shopping](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14261/13975)|AAAI 2017| 1. Present a general solution towards building task-oriented dialogue systems for online shopping, aiming to assist online customers in completing various purchase-related tasks, such as searching products and answering questions, in a natural language conversation manner. 2. Show what & how existing NLP techniques, data resources, and crowdsourcing can be leveraged to build such task-oriented dialogue systems for E-commerce usage.|
|[End-to-End Task-Completion Neural Dialogue Systems](https://arxiv.org/pdf/1703.01008)|IJCNLP 2017| 1. Presents a novel end-to-end learning framework for task-completion dialogue systems. 2. The proposed system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. 3. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system.|
|[Memory-augmented Dialogue Management for Task-oriented Dialogue Systems](https://arxiv.org/pdf/1805.00150)|Arxiv 2018| 1. Propose a novel Memory-Augmented Dialogue management model (MAD) which employs a memory controller and two additional memory structures, i.e., a slot-value memory and an external memory. 2.The slot-value memory tracks the dialogue state by memorizing and updating the values of semantic slots (for instance, cuisine, price, and location), and the external memory augments the representation of hidden states of traditional recurrent neural networks through storing more context information. 3. Propose slot-level attention on user utterances to extract specific semantic information for each slot to update the dialogue state efficiently|
|[UBAR - Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2](https://arxiv.org/abs/2012.03539)|AAAI 2021| 1. Propose UBAR, which is obtained by fine-tuning the large pre-trained unidirectional language model GPT-2 on the sequence of the entire dialog session which is composed of user utterance, belief state, database result, system act, and system response of every dialog turn. 2. UBAR is evaluated in a more realistic setting, where its dialog context has access to user utterances and all content it generated such as belief states, system acts, and system responses. 3. Experimental results on the MultiWOZ datasets show that UBAR achieves state-of-the-art performances in multiple settings, improving the combined score of response generation, policy optimization, and end-to-end modeling by 4.7, 3.5, and 9.4 points respectively.|
|[Template Guided Text Generation for Task-Oriented Dialogue](https://www.aclweb.org/anthology/2020.emnlp-main.527/)|EMNLP 2020| 1. Proposes the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API, where a pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. 2. Show that the proposed method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.|
|[Task-Oriented Dialogue as Dataflow Synthesis](https://arxiv.org/abs/2009.11423)|TACL 2020| 1. Describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph, where a dialogue agent maps each user utterance to a program that extends this graph. 2. The graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. 3. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues.|
|[Probing Task-Oriented Dialogue Representation from Language Models](https://arxiv.org/abs/2010.13912)|EMNLP 2020| 1. Investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks from two aspects: supervised classifier probe and unsupervised mutual information probe. 2. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.|
|[MinTL - Minimalist Transfer Learning for Task-Oriented Dialogue Systems](https://arxiv.org/abs/2009.12005)|EMNLP 2020| 1. Propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. 2. Introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. 3. Experiments demonstrate the effectiveness of the proposed methods.|
|[Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems](https://arxiv.org/abs/2008.06239)|Arxiv 2020| 1. Evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks.|
|[How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue](https://www.aclweb.org/anthology/2020.emnlp-main.230/)|EMNLP 2020| 1. Propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability in NLG of TOD. 2. The system trained using this approach scored 100% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.|
|[SOLOIST - Few-shot Task-Oriented Dialog with A Single Pre-trained Auto-regressive Model](https://arxiv.org/abs/2005.05298)|Arxiv 2020| 1. Presents a new method SOLOIST, which uses transfer learning to efficiently build task-oriented dialog systems at scale, which parameterize a dialog system using a Transformer-based auto-regressive language model, which subsumes different dialog modules (e.g., state tracker, dialog policy, response generator) into a single neural model. 2. Demonstrate that (i) SOLOIST creates new state-of-the-art results on two well-known benchmarks, CamRest and MultiWOZ, (ii) in the few-shot learning setting, the dialog systems developed by SOLOIST significantly outperform those developed by existing methods, and (iii) the use of machine teaching substantially reduces the labeling cost.|
|[A Simple Language Model for Task-Oriented Dialogue](https://arxiv.org/abs/2005.00796)|NeurIPS 2020| 1. Propose a simple approach to task-oriented dialogue that uses a single causal language model trained on all sub-tasks recast as a single sequence prediction problem.|
|[TOD-BERT - Pre-trained Natural Language Understanding for Task-Oriented Dialogue](https://arxiv.org/abs/2004.06871)|EMNLP 2020| 1. Unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. 2. The proposed TOD-BERT model outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. 3. Also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExNDI5NzE1OTcsLTIwNjgzNjUwMTFdfQ
==
-->