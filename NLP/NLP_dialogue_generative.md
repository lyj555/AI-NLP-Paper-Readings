# NLP - Dialogue System (Open-domain, Generative)
|Paper|Conference|Remarks
|--|--|--|
|[A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](https://arxiv.org/pdf/1506.06714)|NAACL 2015|Integrate context information into dialogue generation|
|[A Neural Conversational Model](https://arxiv.org/pdf/1506.05869)|Arxiv 2015|1. Present a simple approach for conversation modelling which uses the recently proposed sequence to sequence framework. 2. It can be trained end-to-end and thus requires much fewer hand-crafted rules|
|[A Persona-Based Neural Conversation Model](http://www.aclweb.org/anthology/P16-1094)|ACL 2016|1. Present persona-based models for handling the issue of speaker consistency in neural response generation. 2. Yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.|
|[Deep Reinforcement Learning for Dialogue Generation](https://aclweb.org/anthology/D16-1127)|EMNLP 2016|1. Current conversational agents tend to be short-sighted and ignoring long-term interactivity. 2. Combination of traditional NLP model with reinforcement learning to model future rewards in chatbot dialogue to learn a neural conversational model based on long-term success of dialogues. 3. Rewards: informative, coherence and ease-of-answering. 4. Model evaluation metrices: diversity, conversation length and human judges|
|[How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation](https://aclweb.org/anthology/D16-1230)|EMNLP 2016|1. Investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. 2. Show that current automatic metrics correlate very weakly with human judgement in the non-technical Twitter domain, and not at all in the technical Ubuntu domain|
|[Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160)|AAAI 2016|1. Extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. 2. Investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings|
|[Topic Aware Neural Response Generation](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14563/14260)|AAAI 2016|1. Consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots. 2. Utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation, and leverages the topic information in generation by a joint attention mechanism and a biased generation probability.|
|[A Context-aware Natural Language Generator for Dialogue Systems](https://arxiv.org/pdf/1608.07076)|SIGDIAL 2016|Present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses.|
|[A Simple, Fast Diverse Decoding Algorithm for Neural Generation](https://arxiv.org/pdf/1611.08562)|Arxiv 2016|1. Propose a simple, fast decoding algorithm that fosters diversity in neural generation. 2. Diverse decoding helps across tasks of dialogue response generation, abstractive summarization and machine translation, especially those for which reranking is needed|
|[A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues](https://arxiv.org/pdf/1605.06069)|Arxiv 2016|1. Propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps to model complex dependencies between subsequences.|
|[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424)|Arxiv 2016|1. Propose DBS to decode a list of diverse outputs by optimizing for a diversity-augmented objective. 2. Observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space|
|[Generative Deep Neural Networks for Dialogue: A Short Review](https://arxiv.org/pdf/1611.06216)|Arxiv 2016|Review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.|
|[Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders](https://arxiv.org/pdf/1703.10960)|ACL 2017| 1. Present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. The proposed model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. 2. Further develop a novel variant that is integrated with linguistic prior knowledge for better performance. 3. Propose a bag-of-word loss to alleviate the problem of vanishing KL loss.|
|[Adversarial Learning for Neural Dialogue Generation](https://www.aclweb.org/anthology/D17-1230)|EMNLP 2017| 1. Propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. 2. Cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. 3. Some appealing research directions.|
|[Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models](https://arxiv.org/pdf/1701.03185)|EMNLP 2017| 1. Add self-attention to the decoder to maintain coherence in longer responses, and propose a practical approach, called the glimpse-model, for scaling to large datasets. 2. Introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process.|
|[ParlAI: A Dialog Research Software Platform](http://aclweb.org/anthology/D17-2014)|EMNLP 2017| A unified open-source framework in Python for sharing, training and testing of dialog models, integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning; and a repository of machine learning models for comparing with others' models, and improving upon existing architectures.|
|[A Survey on Dialogue Systems: Recent Advances and New Frontiers](http://www.kdd.org/exploration_files/19-2-Article3.pdf)|ACM SIGKDD Explorations Newsletter 2017| 1. Task-oriented and non-task oriented models. 2. How deep learning help the representation. 3. Some appealing research directions.|
|[Emotional Poetry Generation](https://pdfs.semanticscholar.org/d89d/053b1c2481088b1af2bd36e0a6d959ff1373.pdf)|SPECOM 2017| 1. Describe a new system for the automatic creation of poetry in Basque that not only generates novel poems, but also creates them conveying a certain attitude or state of mind. 2. The proposed system receives as an input the topic of the poem and the affective state (positive, neutral or negative) and tries to give as output a novel poem that: (1) satisfies formal constraints of rhyme and metric, (2) shows coherent content related to the given topic, and (3) expresses them through the predetermined mood.|
|[Emotional Human-Machine Conversation Generation Based on Long Short-Term Memory](https://link.springer.com/article/10.1007/s12559-017-9539-4)|Cognitive Computation 2017| 1. Propose a new model based on long short-term memory, which is used to achieve an encoder-decoder framework, and we address the emotional factor of conversation generation by changing the model’s input using a series of input transformations: a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses.|
|[A Deep Reinforcement Learning Chatbot](https://arxiv.org/pdf/1709.02349)|Arxiv 2017| 1. Consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. 2. The system has been trained to select an appropriate response from the models in its ensemble.|
|[Building Chatbot with Emotions](http://web.stanford.edu/class/cs224s/reports/Honghao_Wei.pdf)|N.A. 2017| 1. Aims at generating dialogues not only appropriate at content level, but also containing specific emotions. 2. Apply sentimental analysis on the dataset and pick up dialogue with strong emotion. 3. Apply deep reinforcement learning and introduce sentiment rewards during learning phase|
|[Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base](https://papers.nips.cc/paper/7558-dialog-to-action-conversational-question-answering-over-a-large-scale-knowledge-base.pdf)|NIPS 2018|1. Present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. 2. Introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances.|
|[Knowledge Diffusion for Neural Dialogue Generation](http://www.aclweb.org/anthology/P18-1138)|ACL 2018|1. Propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. 2. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities.|
|[Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures](http://aclweb.org/anthology/P18-1133)|ACL 2018|1. Propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. 2. Design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. 3. Propose a simplistic Two Stage CopyNet instantiation which demonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude.|
|[Sentiment Adaptive End-to-End Dialog Systems](https://arxiv.org/pdf/1804.10731)|ACL 2018|1. Propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end dialog learning framework to make systems more user-adaptive and effective. 2. Incorporated user sentiment information in both supervised and reinforcement learning settings and gained improvements on a bus information search task.|
|[MojiTalk: Generating Emotional Responses at Scale](https://arxiv.org/pdf/1711.04090)|ACL 2018|1. Collect a large corpus of Twitter conversations that include emojis in the response, and assume the emojis convey the underlying emotions of the sentence. 2. Introduce a reinforced conditional variational encoder approach to train a deep generative model on these conversations, which allows us to use emojis to control the emotion of the generated text.|
|[Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems](https://arxiv.org/pdf/1804.08217)|ACL 2018|Propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to incorporate knowledge bases. |
|[Global-Locally Self-Attentive Dialogue State Tracker](https://arxiv.org/pdf/1805.09655)|ACL 2018|1. Propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. 2. The model uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. 3. Show that this significantly improves tracking of rare states and achieves state-of-the-art performance on the WoZ and DSTC2 state tracking tasks.|
|[Variational Autoregressive Decoder for Neural Response Generation](http://aclweb.org/anthology/D18-1354)|EMNLP 2018|1. To solve the generation problem in VAE, this work proposes a novel model that sequentially introduces a series of latent variables to condition the generation of each word in the response sequence. 2. The approximate posteriors of these latent variables are augmented with a backward Recurrent Neural Network (RNN), which allows the latent variables to capture long-term dependencies of future tokens in generation.|
|[Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos](http://www.aclweb.org/anthology/N18-1193)|NAACL 2018|Propose a deep neural framework, termed conversational memory network, which leverages contextual information from the conversation history, in dyadic dialogue videos|
|[Improving Variational Encoder-Decoders in Dialogue Generation](https://arxiv.org/pdf/1802.02032)|AAAI 2018|1. To address the KL-vanishing problem and inconsistent training objective in VAE, they propose to separate the training step into two phases: The first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding.|
|[Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16455/15753)|AAAI 2018|1. Models high-level abstraction of emotion expressions by embedding emotion categories. 2. Captures the change of implicit internal emotion states. 3. Uses explicit emotion expressions with an external emotion vocabulary|
|[Augmenting End-to-End Dialogue Systems with Commonsense Knowledge](https://arxiv.org/pdf/1709.05453)|AAAI 2018| 1. Investigate the impact of providing commonsense knowledge about the concepts covered in the dialog. 2. Propose the Tri-LSTM model to jointly take into account message and commonsense for selecting an appropriate response|
|[A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/pdf/1702.01932)|AAAI 2018| 1. Presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. 2. Generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting.|
|[Neural Response Generation with Dynamic Vocabularies](https://arxiv.org/pdf/1711.11191)|AAAI 2018| 1. Propose a dynamic vocabulary sequence-to-sequence (DVS2S) model which allows each input to possess their own vocabulary in decoding. 2. In training, vocabulary construction and response generation are jointly learned by maximizing a lower bound of the true objective with a Monte Carlo sampling method. 3. In inference, the model dynamically allocates a small vocabulary for an input with the word prediction model, and conducts decoding only with the small vocabulary.|
|[Improving Variational Encoder-Decoders in Dialogue Generation](https://arxiv.org/pdf/1802.02032)|AAAI 2018| In VAE, encoder and decoder training are inconsistent, this work separate two phases: the first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding.|
|[A Hierarchical Latent Structure for Variational Conversation Modeling](https://arxiv.org/pdf/1804.03424)|NAACL 2018| 1. VAE for dialogue generation suffers from the generation problems: (1) the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables; (2) the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables. 2. To solve the generation problem, this paper proposes a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization.|
|[Dialog Generation Using Multi-Turn Reasoning Neural Networks](https://arxiv.org/pdf/1804.03424)|NAACL 2018| Propose a generalizable dialog generation approach that adapts multiturn reasoning, one recent advancement in the field of document comprehension, to generate responses (“answers”) by taking current conversation session context as a “document” and current query as a “question”.|
|[Affective Neural Response Generation](https://arxiv.org/abs/1709.03968)|ECIR 2018| 1.  Propose three novel ways to incorporate affective aspects into LSTM encoder-decoder neural conversation models: Affective word embeddings, affect-based objective functions, affectively diverse beam search for decoding. 2. Experiments show that the proposed model produce emotionally rich responses that are more interesting and natural|
|[Towards Neural Speaker Modeling in Multi-Party Conversation: The Task, Dataset, and Models](https://arxiv.org/pdf/1708.03152)|LREC 2018| 1. Propose speaker classification as a surrogate task for general speaker modeling, and collect massive data to facilitate research in this direction.|
|[WIZARD OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS](https://openreview.net/pdf?id=r1l73iRqKm)|ICLR 2019| 1. Collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. 2. Design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses.|
|[Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs](https://arxiv.org/pdf/1812.04616)|ICLR 2019| 1. Propose a general technique for replacing the softmax layer with a continuous embedding layer. 2. Proposes a novel probabilistic loss, and a training and inference procedure in which we generate a probability distribution over pre-trained word embeddings, instead of a multinomial distribution over the vocabulary obtained via softmax.|
|[Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models](http://proceedings.mlr.press/v97/cohen19a.html)|ICML 2019| 1. Perform an empirical study of the behavior of beam search across three sequence synthesis tasks. 2. Find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. 3. Show that constraining beam search to avoid large discrepancies eliminates the performance degradation.|
|[Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References](https://www.aclweb.org/anthology/P19-1372.pdf)|ACL 2019| 1. Propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. 2. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses.|
|[Dialogue Natural Language Inference](https://arxiv.org/pdf/1811.00671)|ACL 2019| 1. Frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. 2. Propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model.|
|[Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study](https://arxiv.org/pdf/1906.01603)|ACL 2019| 1. Take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artificially introduced unnatural changes or perturbations to their context at test time. 2. Experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shuffling words, etc.|
|[Domain Adaptive Dialog Generation via Meta Learning](https://arxiv.org/pdf/1906.03520)|ACL 2019| 1. Propose a domain adaptive dialog generation method based on meta-learning (DAML), an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. 2. Train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain.|
|[Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://arxiv.org/pdf/1906.07004)|ACL 2019| 1. Propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. 2. Collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network.|
|[OpenDialKG - Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs](https://research.fb.com/wp-content/uploads/2019/07/OpenDialKG-Explainable-Conversational-Reasoning-with-Attention-based-Walks-over-Knowledge-Graphs.pdf?)|ACL 2019| 1. Collect a new Open-ended DialogKG parallel corpus called _OpenDialKG_, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. 2. Propose the _DialKG Walker_ model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder.|
|[Personalizing Dialogue Agents via Meta-Learning](https://arxiv.org/pdf/1905.10033)|ACL 2019| 1. Propose to extend Model-Agnostic Meta-Learning (MAML)(Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. 2. The model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions.|
|[Persuasion for Good - Towards a Personalized Persuasive Dialogue System for Social Good](https://arxiv.org/pdf/1906.06725)|ACL 2019| 1. Design an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. 2. Collect a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. 3. Build a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. 4. Analyze the relationships between individuals' demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation.|
|[Structuring Latent Spaces for Stylized Response Generation](https://arxiv.org/pdf/1909.05361)|EMNLP 2019| 1. Propose StyleFusion, which bridges conversation modeling and non-parallel style transfer by sharing a structured latent space. 2. This structure allows the system to generate stylized relevant responses by sampling in the neighborhood of the conversation model prediction, and continuously control the style level.|
|[Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs](https://arxiv.org/pdf/1903.10245)|EMNLP 2019| 1. Propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. 2. Formulate the knowledge selection on the graph as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous work.|
|[DyKgChat - Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs](https://arxiv.org/pdf/1910.00610)|EMNLP 2019| 1. Proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. 2. Proposes a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs.|
|[Gunrock - A Social Bot for Complex and Engaging Long Conversations](https://arxiv.org/pdf/1910.03042)|EMNLP 2019| 1. The winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazon-selected expert conversationalists. 2. Introduce some innovative system designs and related validation analysis.|
|[Linguistically-Informed Specificity and Semantic Plausibility for Dialogue Generation](https://www.aclweb.org/anthology/N19-1349/)|NAACL 2019| 1. Examine whether specificity is solely a frequency-related notion and find that more linguistically-driven specificity measures are better suited to improving response informativeness. 2. Find that forcing a sequence-to-sequence model to be more specific can expose a host of other problems in the responses, including flawed discourse and implausible semantics.|
|[A Survey of Natural Language Generation Techniques with a Focus on Dialogue Systems - Past, Present and Future Directions](https://arxiv.org/pdf/1906.00500)|Arxiv 2019| 1. Provide a comprehensive review towards building open domain dialogue systems, an important application of natural language generation. 2. Find that, predominantly, the approaches for building dialogue systems use seq2seq or language models architecture. Notably, we identify three important areas of further research towards building more effective dialogue systems: 1) incorporating larger context, including conversation context and world knowledge; 2) adding personae or personality in the NLG system; and 3) overcoming dull and generic responses that affect the quality of system-produced responses.|
|[The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751)|ICLR 2020| 1. Reveal surprising distributional differences between human text and machine text. 2. Find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. 3. Propose Nucleus Sampling, a simple but effective method to draw the best out of neural generation.|
|[MIME - MIMicking Emotions for Empathetic Response Generation](https://arxiv.org/abs/2010.01454)|EMNLP 2020|1. Argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. 2. Show that the consideration of this polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. 3. Introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. 4. Demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations.|
|[Learning to Respond with Stickers - A Framework of Unifying Multi-Modality in Multi-Turn Dialog](https://arxiv.org/abs/2003.04679)|WWW 2020|1. Propose to recommend an appropriate sticker to user based on multi-turn dialog context history without any external labels. 2. Collect a large-scale real-world dialog dataset with stickers from one of the most popular online chatting platform.|
|[Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks](https://arxiv.org/abs/1910.14326)|ACL 2020|1. Propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting, where each dialogue model consists of a shared module, a gating module, and a private module. 2. Show that the proposed method outperforms all the baselines in terms of task consistency, response quality, and diversity.|
|[Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks](https://arxiv.org/abs/2004.01972)|EMNLP 2020|1. Propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. 2. Show that the model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.|
|[Leading Conversational Search by Suggesting Useful Questions](https://www.microsoft.com/en-us/research/publication/leading-conversational-search-by-suggesting-useful-questions/)|WWW 2020|1. Studies a new scenario in conversational search, conversational question suggestion, which leads search engine users to more engaging experiences by suggesting interesting, informative, and useful follow-up questions. 2. Propose a new metric, usefulness, and construct a benchmark dataset for useful question suggestion. 3. Develop two suggestion systems, a BERT ranking model and a GPT-2 generation model, both trained in a multi-task learning framework. 4. Online epxeriments show that more useful question suggestions receive 8% more user clicks than the previous system.|
|[Knowledge-Grounded Dialogue Generation with Pre-trained Language Models](https://arxiv.org/abs/2010.08824)|EMNLP  2020|1. Propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. 2. Show that the model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.|
|[Information Seeking in the Spirit of Learning - a Dataset for Conversational Curiosity](https://arxiv.org/abs/2005.00172)|EMNLP 2020|1. Design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know. 2. Collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations.|
|[Generating Dialogue Responses from a Semantic Latent Space](https://arxiv.org/abs/2010.01658)|EMNLP 2020|1. Hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. 2. Propose to learn the pair relationship between the prompts and responses as a regression task on a latent space instead. 3. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.|
|[Evaluating Dialogue Generation Systems via Response Selection](https://arxiv.org/abs/2004.14302)|ACL 2020|1. Propose the method to construct response selection test sets with well-chosen false candidates. 2. Demonstrate that evaluating systems via response selection with the test sets developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU.|
|[Diversifying Dialogue Generation with Non-Conversational Text](https://arxiv.org/abs/2005.04346)|ACL 2020|1. Propose a new perspective to diversify dialogue generation by leveraging non-conversational text. 2. Present a training paradigm to effectively incorporate these text via iterative back translation. 3. Show that the model produces significantly more diverse responses without sacrificing the relevance with context.|
|[Dialogue Response Ranking Training with Large-Scale Human Feedback Data](https://arxiv.org/abs/2009.06978)|EMNLP 2020|1. Leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. 2. Trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines.|
|[Designing Precise and Robust Dialogue Response Evaluators](https://arxiv.org/abs/2004.04908)|ACL 2020|1. Propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. 2. Demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes robustly to diverse responses and corpora.|
|[Conversational Word Embedding for Retrieval-Based Dialog System](https://arxiv.org/abs/2004.13249)|ACL 2020|1. Propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs ⟨post,reply⟩ to learn word embedding. 2. Show that PR-Embedding can improve the quality of the selected response.|
|[CHARM - Inferring Personal Attributes from Conversations](https://www.aclweb.org/anthology/2020.emnlp-main.434/)|EMNLP 2020|1. Propose CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training.|
|[A Large-Scale Chinese Short-Text Conversation Dataset](https://arxiv.org/abs/2008.03946)|NLPCC 2020|1. Present a large-scale cleaned Chinese conversation dataset, LCCC, which contains a base version (6.8million dialogues) and a large version (12.0 million dialogues). 2. Release pre-training dialogue models which are trained on LCCC-base and LCCC-large respectively.|
|[“None of the Above” - Measure Uncertainty in Dialog Response Retrieval](https://www.aclweb.org/anthology/2020.acl-main.182/)|ACL 2020|1. Discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus. 2. Show that instead of retraining models for this specific purpose, authors can capture the original retrieval model’s underlying confidence concerning the best prediction using trivial additional computation.|

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4NzQzNjQ4MzIsLTEyNzEwMzAzODQsMT
U0NzE1ODQ5NCwtMzg3NTI3OCwtNDA4MzE4NDYwXX0=
-->